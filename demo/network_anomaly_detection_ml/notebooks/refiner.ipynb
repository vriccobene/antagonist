{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the telemetry data from InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import influx_db_utils as influx\n",
    "\n",
    "# TOTAL_LEN_OF_DATA = 40  # Total number of days of data in the DB\n",
    "\n",
    "group = \"Group-1\"\n",
    "machine_id = 'machine-1-1'\n",
    "# training_data_len = 20  # Number of days of data to use for training\n",
    "\n",
    "# Read historical data from InfluxDB (the first 20 days of data in the DB)\n",
    "# now = datetime.datetime.now()\n",
    "# end = now - datetime.timedelta(days=1)\n",
    "# start = end - datetime.timedelta(days=20)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "start = end - datetime.timedelta(days=365)\n",
    "\n",
    "db = influx.SMDInfluxDB()\n",
    "dataframes, machines = db.read_dataset(\n",
    "    start_date=start,\n",
    "    end_date=end,\n",
    "    machine_name=machine_id,\n",
    ")\n",
    "historical_telemetry_df = dataframes[0]  # pandas.DataFrame\n",
    "historical_telemetry_df = historical_telemetry_df[\n",
    "    historical_telemetry_df.columns[1:].tolist()+['timestamp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter up to current day to simulate the predition on the next one\\n\",\n",
    "current_day = historical_telemetry_df['timestamp'].min() + datetime.timedelta(days=36)\n",
    "next_day = current_day + datetime.timedelta(days=1)\n",
    "\n",
    "historical_telemetry_df = dataframes[0]\n",
    "historical_telemetry_df = historical_telemetry_df[historical_telemetry_df['timestamp']<current_day.ctime()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the anomaly labels validated by network experts (from Antagonist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def format_symptoms(symptoms_json, start, end):\n",
    "    # TODO: Move the filtering capabilities inside Antagonist\n",
    "    annotator_type = \"human\"\n",
    "    tags={\"machine\": machine_id}\n",
    "    start_time = datetime.datetime.strptime(symptom['start-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "    end_time = datetime.datetime.strptime(symptom['end-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "    \n",
    "    symptoms = []\n",
    "    for symptom in symptoms_json:\n",
    "        \n",
    "        # TODO: Move the filtering capabilities inside Antagonist\n",
    "\n",
    "        # verify overlap between symptom interval and filters one\n",
    "        time_overlap = (start.timestamp() <= start_time <= end.timestamp()) or (start.timestamp() <= end_time <= end.timestamp())\n",
    "        if (annotator_type is None or symptom[\"annotator_type\"] == annotator_type) and time_overlap:\n",
    "            if tags is None or all([symptom[\"tags\"][tag] == tags[tag] for tag in tags]):\n",
    "                symptom.update({\n",
    "                    \"start-time\": start_time,\n",
    "                    \"end-time\": end_time\n",
    "                })\n",
    "                symptoms.append(symptom)\n",
    "    return symptoms\n",
    "\n",
    "\n",
    "ANTAGONIST_HOST = \"localhost:5001\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer eyJrIjoiZnVMc2pWeFkwNG81MmlxVUVJUWY2T20yblh6ZzkxUTEiLCJuIjoiYW50YWdvbmlzdCIsImlkIjoxfQ==\"\n",
    "})\n",
    "headers = {\n",
    "'Authorization': 'Bearer glsa_jpNMmdn5d1lvuVbdc8f4cQ2Q0WTsAJbh_55df2413',\n",
    "'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"start_time\": start.strftime(\"%Y/%m/%dT%H:%M\"),\n",
    "    \"end_time\": end.strftime(\"%Y/%m/%dT%H:%M\"),\n",
    "    \"annotator-type\": \"human\"\n",
    "}\n",
    "response = requests.request(\n",
    "    \"GET\", f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\", \n",
    "    headers=headers, data=payload, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "# symptoms = format_symptoms(response.json(), start, end)\n",
    "symptoms = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def json_to_df(symptoms):\n",
    "    annotation_df = pd.DataFrame()\n",
    "    annotation_df['timestamp'] = historical_telemetry_df['timestamp']\n",
    "    annotation_df['label'] = 0\n",
    "\n",
    "    for symptom in symptoms:\n",
    "        start_time_epoch = pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "        end_time_epoch = pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "        \n",
    "        if (end_time_epoch - start_time_epoch) > 86400:\n",
    "            # Skip anomalies bigger than 1 day\n",
    "            continue\n",
    "\n",
    "        annotation_df.loc[\n",
    "            (annotation_df['timestamp'] >= pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\")) &\n",
    "            (annotation_df['timestamp'] <= pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\"))\n",
    "        , 'label'] = 1\n",
    "    return annotation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the current anomaly detector and generate the detection labels on the current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from demo_anomaly_detector import autoencoder_detector\n",
    "current_anomaly_detector = autoencoder_detector.DemoAnomalyDetector()\n",
    "network_anomalies_current_model = current_anomaly_detector.detect(historical_telemetry_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Anomaly Detector object and train it on the labels that were retrieved from Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Stored the model: ae_model_1721592388.2230747\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from demo_anomaly_detector import autoencoder_detector\n",
    "new_anomaly_detector = autoencoder_detector.DemoAnomalyDetector()\n",
    "\n",
    "annotation_df = json_to_df(symptoms)\n",
    "new_anomaly_detector.train(historical_telemetry_df, annotation_df, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "network_anomalies_new_model = new_anomaly_detector.detect(historical_telemetry_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the detector that was just trained with the one currently \"in production\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "def estimate_performance(network_anomalies, annotation_df):\n",
    "    res = copy.deepcopy(annotation_df)\n",
    "    res.set_index('timestamp', inplace=True)\n",
    "    for network_anomaly in network_anomalies:\n",
    "        for symptom in network_anomaly[2]:\n",
    "            symptom_start_time = pd.Timestamp(symptom[1], unit=\"s\", tz=\"UTC\")\n",
    "            symptom_end_time = pd.Timestamp(symptom[2], unit=\"s\", tz=\"UTC\")\n",
    "            symptom_df = pd.DataFrame({'timestamp': pd.date_range(start=symptom_start_time, end=symptom_end_time, freq='1min', tz=\"UTC\"), 'predicted_label': 1})\n",
    "            symptom_df.set_index('timestamp', inplace=True)\n",
    "            res = pd.concat([res, symptom_df], sort=False)\n",
    "    false_positives = res[(res['label'] != res['predicted_label']) & (pd.isnull(res['label']))]\n",
    "    false_negatives = res[(res['label'] != res['predicted_label']) & (pd.isnull(res['predicted_label']))]\n",
    "    return len(false_positives), len(false_negatives)\n",
    "\n",
    "current_fp, current_fn = estimate_performance(network_anomalies_current_model, annotation_df)\n",
    "new_fp, new_fn = estimate_performance(network_anomalies_new_model, annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: FP=11223, FN=51840\n",
      "New model: FP=11223, FN=51840\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current model: FP={current_fp}, FN={current_fn}\")\n",
    "print(f\"New model: FP={new_fp}, FN={new_fn}\")\n",
    "\n",
    "# Very simple comparison strategy to select the best model\n",
    "if new_fp + new_fn < current_fp + current_fn:\n",
    "    network_anomalies = network_anomalies_new_model\n",
    "    anomaly_detector = new_anomaly_detector\n",
    "else:\n",
    "    # If the new model is not selected, it needs to be deleted\n",
    "    new_anomaly_detector.delete()\n",
    "    anomaly_detector = current_anomaly_detector\n",
    "    network_anomalies = network_anomalies_current_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the labels for the detected symptoms into Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_network_anomalies_for_antagonist(network_anomalies, tags, model_name):\n",
    "    res = list()\n",
    "    for anomaly in network_anomalies:\n",
    "        symptoms = list()\n",
    "        for symptom in anomaly[2]:\n",
    "            tags = tags\n",
    "            tags[\"metric\"] = db.get_metric_names()[symptom[0]]\n",
    "            symptom_dict = {\n",
    "                # 'start-time': start.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                # 'end-time': end.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                'start-time': datetime.datetime.fromtimestamp(symptom[1]).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                'end-time': datetime.datetime.fromtimestamp(symptom[2]).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                \"concern-score\": symptom[3],\n",
    "                \"confidence-score\": symptom[4],\n",
    "                \"description\": \"Symptom\",\n",
    "                \"annotator\": {\n",
    "                    \"name\": f\"{model_name}\",\n",
    "                    \"annotator_type\": \"algorithm\"\n",
    "                },\n",
    "                \"tags\": tags,\n",
    "                \"pattern\": \"\",\n",
    "            }\n",
    "            symptoms.append(symptom_dict)\n",
    "\n",
    "        anomaly_dict = {\n",
    "            \"annotator\": {\n",
    "                \"name\": anomaly_detector.get_model_name(),\n",
    "                \"annotator_type\": \"algorithm\",\n",
    "            },\n",
    "            \"description\": f'Detected Network Anomaly on {machine_id} - {datetime.datetime.fromtimestamp(anomaly[0]).strftime(\"%Y-%m-%d at %H\")}',\n",
    "            \"state\": \"problem-potential\",\n",
    "            \"version\": 1,\n",
    "            \"symptoms\": symptoms\n",
    "        }\n",
    "        res.append(anomaly_dict)\n",
    "    return res\n",
    "\n",
    "\n",
    "tags = {'machine': machine_id, 'group': group}\n",
    "anomalies_to_store = format_network_anomalies_for_antagonist(network_anomalies, tags, anomaly_detector.get_model_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send data to Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Send the data to Antagonist\n",
    "for network_anomaly in anomalies_to_store:\n",
    "    symptoms = network_anomaly.pop(\"symptoms\")\n",
    "    response = requests.post(\n",
    "        f\"http://{ANTAGONIST_HOST}/api/rest/v1/network_anomaly\", json=network_anomaly\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    network_anomaly_id = response.json()\n",
    "\n",
    "    # Create network symptoms labels and link with the network anomaly\n",
    "    for symptom in symptoms:\n",
    "        symptom['event-id'] = network_anomaly_id\n",
    "        # Persist the Symptom\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\", json=symptom\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        symptom_uuid = response.json()\n",
    "\n",
    "        # Link the Symptom to the network anomaly\n",
    "        sym_to_net = {\"symptom-id\": symptom_uuid, \"network-anomaly-id\": network_anomaly_id}\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/network_anomaly/symptom\", json=symptom\n",
    "        )\n",
    "        response.raise_for_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
