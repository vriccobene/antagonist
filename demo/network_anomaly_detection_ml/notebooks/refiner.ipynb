{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the telemetry data from InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import influx_db_utils as influx\n",
    "\n",
    "# TOTAL_LEN_OF_DATA = 40  # Total number of days of data in the DB\n",
    "\n",
    "group = \"Group-1\"\n",
    "machine_id = 'machine-1-1'\n",
    "# training_data_len = 20  # Number of days of data to use for training\n",
    "\n",
    "# Read historical data from InfluxDB\n",
    "end = datetime.datetime.now()\n",
    "start = end - datetime.timedelta(days=365)\n",
    "\n",
    "db = influx.SMDInfluxDB()\n",
    "dataframes, machines = db.read_dataset(\n",
    "    start_date=start,\n",
    "    end_date=end,\n",
    "    machine_name=machine_id,\n",
    ")\n",
    "historical_telemetry_df = dataframes[0]  # pandas.DataFrame\n",
    "historical_telemetry_df = historical_telemetry_df[\n",
    "    historical_telemetry_df.columns[1:].tolist()+['timestamp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter up to current day to simulate the predition on the next one\n",
    "current_day = historical_telemetry_df['timestamp'].min() + datetime.timedelta(days=36)\n",
    "next_day = current_day + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_telemetry_df = dataframes[0]\n",
    "historical_telemetry_df = historical_telemetry_df[historical_telemetry_df['timestamp']<current_day.ctime()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the anomaly labels validated by network experts (from Antagonist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def format_symptoms(symptoms_json, start, end):\n",
    "    # TODO: Move the filtering capabilities inside Antagonist\n",
    "    source_type = \"human\"\n",
    "    tags={\"machine\": machine_id}\n",
    "    \n",
    "    symptoms = []\n",
    "    for symptom in symptoms_json:\n",
    "        start_time = datetime.datetime.strptime(symptom['start-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "        end_time = datetime.datetime.strptime(symptom['end-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "\n",
    "        # TODO: Move the filtering capabilities inside Antagonist\n",
    "\n",
    "        # verify overlap between symptom interval and filters one\n",
    "        time_overlap = (start.timestamp() <= start_time <= end.timestamp()) or (start.timestamp() <= end_time <= end.timestamp())\n",
    "        if (source_type is None or symptom[\"source-type\"] == source_type) and time_overlap:\n",
    "            if tags is None or all([symptom[\"tags\"][tag] == tags[tag] for tag in tags]):\n",
    "                symptom.update({\n",
    "                    \"start-time\": start_time,\n",
    "                    \"end-time\": end_time\n",
    "                })\n",
    "                symptoms.append(symptom)\n",
    "    return symptoms\n",
    "\n",
    "\n",
    "ANTAGONIST_HOST = \"localhost:5001\"\n",
    "response = requests.get(f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\")\n",
    "response.raise_for_status()\n",
    "\n",
    "# symptoms = format_symptoms(response.json(), start, end)\n",
    "symptoms = response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "annotation_df = pd.DataFrame()\n",
    "annotation_df['timestamp'] = historical_telemetry_df['timestamp']\n",
    "annotation_df['label'] = 0\n",
    "\n",
    "for symptom in symptoms:\n",
    "    start_time_epoch = pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "    end_time_epoch = pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "    \n",
    "    if (end_time_epoch - start_time_epoch) > 86400:\n",
    "        # Skip anomalies bigger than 1 day\n",
    "        continue\n",
    "\n",
    "    # print(symptom['start-time'])\n",
    "    # print(symptom['end-time'])\n",
    "\n",
    "    annotation_df.loc[\n",
    "        (annotation_df['timestamp'] >= pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\")) &\n",
    "        (annotation_df['timestamp'] <= pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\"))\n",
    "    , 'label'] = 1\n",
    "    # print(annotation_df[annotation_df['label'] == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df[annotation_df['label'] == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the current anomaly detector and generate the detection labels on the current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from demo_anomaly_detector import autoencoder_detector\n",
    "current_anomaly_detector = autoencoder_detector.DemoAnomalyDetector()\n",
    "network_anomalies_current_model = current_anomaly_detector.detect(historical_telemetry_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Anomaly Detector object and train it on the labels that were retrieved from Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from demo_anomaly_detector import autoencoder_detector\n",
    "new_anomaly_detector = autoencoder_detector.DemoAnomalyDetector()\n",
    "new_anomaly_detector.train(historical_telemetry_df, annotation_df, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_anomalies_new_model = new_anomaly_detector.detect(historical_telemetry_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the detector that was just trained with the one currently \"in production\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "def estimate_performance(network_anomalies, annotation_df):\n",
    "    res = copy.deepcopy(annotation_df)\n",
    "    res.set_index('timestamp', inplace=True)\n",
    "    for network_anomaly in network_anomalies:\n",
    "        for symptom in network_anomaly[2]:\n",
    "            symptom_start_time = pd.Timestamp(symptom[1], unit=\"s\", tz=\"UTC\")\n",
    "            symptom_end_time = pd.Timestamp(symptom[2], unit=\"s\", tz=\"UTC\")\n",
    "            symptom_df = pd.DataFrame({'timestamp': pd.date_range(start=symptom_start_time, end=symptom_end_time, freq='1min', tz=\"UTC\"), 'predicted_label': 1})\n",
    "            symptom_df.set_index('timestamp', inplace=True)\n",
    "            res = pd.concat([res, symptom_df], sort=False)\n",
    "    false_positives = res[(res['label'] != res['predicted_label']) & (pd.isnull(res['label']))]\n",
    "    false_negatives = res[(res['label'] != res['predicted_label']) & (pd.isnull(res['predicted_label']))]\n",
    "    return len(false_positives), len(false_negatives)\n",
    "\n",
    "current_fp, current_fn = estimate_performance(network_anomalies_current_model, annotation_df)\n",
    "new_fp, new_fn = estimate_performance(network_anomalies_new_model, annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current model: FP={current_fp}, FN={current_fn}\")\n",
    "print(f\"New model: FP={new_fp}, FN={new_fn}\")\n",
    "\n",
    "# Very simple comparison strategy to select the best model\n",
    "if new_fp + new_fn < current_fp + current_fn:\n",
    "    network_anomalies = network_anomalies_new_model\n",
    "    anomaly_detector = new_anomaly_detector\n",
    "else:\n",
    "    # If the new model is not selected, it needs to be deleted\n",
    "    new_anomaly_detector.delete()\n",
    "    anomaly_detector = current_anomaly_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the labels for the detected symptoms into Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_network_anomalies_for_antagonist(network_anomalies, tags, model_name):\n",
    "    res = list()\n",
    "    for anomaly in network_anomalies:\n",
    "        symptoms = list()\n",
    "        for symptom in anomaly[2]:\n",
    "            tags = tags\n",
    "            tags[\"metric\"] = db.get_metric_names()[symptom[0]]\n",
    "            symptom_dict = {\n",
    "                'start-time': datetime.datetime.fromtimestamp(symptom[1]).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                'end-time': datetime.datetime.fromtimestamp(symptom[2]).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                \"concern-score\": symptom[3],\n",
    "                \"confidence-score\": symptom[4],\n",
    "                \"description\": \"Symptom\",\n",
    "                \"source-name\": f\"{model_name}\",\n",
    "                \"source-type\": \"algorithm\",\n",
    "                \"tags\": tags,\n",
    "                \"action\": \"drop\",\n",
    "                \"cause\": \"x\",\n",
    "                \"reason\": \"x\",\n",
    "                \"plane\": \"forwarding\",\n",
    "                \"pattern\": \"\",\n",
    "            }\n",
    "            symptoms.append(symptom_dict)\n",
    "\n",
    "        anomaly_dict = {\n",
    "            \"author\": {\n",
    "                \"author_type\": \"algorithm\",\n",
    "                \"name\": anomaly_detector.get_model_name(),\n",
    "                \"version\": 1,\n",
    "            },\n",
    "            \"description\": f'Detected Network Anomaly on {machine_id} - {datetime.datetime.fromtimestamp(anomaly[0]).strftime(\"%Y-%m-%d at %H\")}',\n",
    "            \"state\": \"incident-potential\",\n",
    "            \"version\": 1,\n",
    "            \"symptoms\": symptoms\n",
    "        }\n",
    "        res.append(anomaly_dict)\n",
    "    return res\n",
    "\n",
    "\n",
    "tags = {'machine': machine_id, 'group': group}\n",
    "anomalies_to_store = format_network_anomalies_for_antagonist(network_anomalies, tags, anomaly_detector.get_model_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send data to Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Send the data to Antagonist\n",
    "for network_anomaly in anomalies_to_store:\n",
    "    symptoms = network_anomaly.pop(\"symptoms\")\n",
    "    response = requests.post(\n",
    "        f\"http://{ANTAGONIST_HOST}/api/rest/v1/incident\", json=network_anomaly\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    network_anomaly_id = response.json()\n",
    "\n",
    "    # Create network symptoms labels and link with the network incident\n",
    "    for symptom in symptoms:\n",
    "        symptom['event-id'] = network_anomaly_id\n",
    "        # Persist the Symptom\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\", json=symptom\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        symptom_uuid = response.json()\n",
    "\n",
    "        # Link the Symptom to the network anomaly\n",
    "        sym_to_net = {\"symptom-id\": symptom_uuid, \"incident-id\": network_anomaly_id}\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/incident/symptom\", json=symptom\n",
    "        )\n",
    "        response.raise_for_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
