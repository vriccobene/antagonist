{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Anomaly Detector object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from demo_anomaly_detector import autoencoder_detector\n",
    "\n",
    "# If a model has been pre-trained, it will be loaded automatically\n",
    "anomaly_detector = autoencoder_detector.DemoAnomalyDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the telemetry data from InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m start \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m365\u001b[39m)\n\u001b[0;32m     16\u001b[0m db \u001b[38;5;241m=\u001b[39m influx\u001b[38;5;241m.\u001b[39mSMDInfluxDB()\n\u001b[1;32m---> 17\u001b[0m dataframes, machines \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m telemetry_df \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# pandas.DataFrame\u001b[39;00m\n\u001b[0;32m     23\u001b[0m telemetry_df \u001b[38;5;241m=\u001b[39m telemetry_df[\n\u001b[0;32m     24\u001b[0m     telemetry_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;241m+\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\demo\\network_anomaly_detection_ml\\notebooks\\..\\influx_db_utils.py:364\u001b[0m, in \u001b[0;36mSMDInfluxDB.read_dataset\u001b[1;34m(self, start_date, end_date, metric_name, machine_name)\u001b[0m\n\u001b[0;32m    337\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom(bucket: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manomaly_detection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) |> range(start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;241m.\u001b[39misoformat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mZ, stop: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;241m.\u001b[39misoformat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mZ)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     )\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m#print(query)\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mquery_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minflux_org\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\query_api.py:206\u001b[0m, in \u001b[0;36mQueryApi.query\u001b[1;34m(self, query, org, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m org \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_org_param(org)\n\u001b[0;32m    203\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_api\u001b[38;5;241m.\u001b[39mpost_query(org\u001b[38;5;241m=\u001b[39morg, query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_query(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_dialect, params),\n\u001b[0;32m    204\u001b[0m                                       async_req\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, _return_http_data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_query_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\_base.py:238\u001b[0m, in \u001b[0;36m_BaseQueryApi._to_tables\u001b[1;34m(self, response, query_options, response_metadata_mode)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03mParse HTTP response to TableList.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m:param response: HTTP response from an HTTP client. Expected type: `urllib3.response.HTTPResponse`.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m _parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_tables_parser(response, query_options, response_metadata_mode)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parser\u001b[38;5;241m.\u001b[39mtable_list()\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\flux_csv_parser.py:115\u001b[0m, in \u001b[0;36mFluxCsvParser.generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return Python generator.\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m parser:\n\u001b[1;32m--> 115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_flux_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\flux_csv_parser.py:126\u001b[0m, in \u001b[0;36mFluxCsvParser._parse_flux_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m metadata \u001b[38;5;241m=\u001b[39m _FluxCsvParserMetadata()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader:\n\u001b[1;32m--> 126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_flux_response_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Return latest DataFrame\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\flux_csv_parser.py:228\u001b[0m, in \u001b[0;36mFluxCsvParser._parse_flux_response_row\u001b[1;34m(self, metadata, csv)\u001b[0m\n\u001b[0;32m    225\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mtable_index \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mtable_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    226\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mtable_id \u001b[38;5;241m=\u001b[39m current_id\n\u001b[1;32m--> 228\u001b[0m flux_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_profiler_record(flux_record):\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_profiler_info(flux_record)\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\flux_csv_parser.py:265\u001b[0m, in \u001b[0;36mFluxCsvParser.parse_record\u001b[1;34m(self, table_index, table, csv)\u001b[0m\n\u001b[0;32m    263\u001b[0m     column_name \u001b[38;5;241m=\u001b[39m fluxColumn\u001b[38;5;241m.\u001b[39mlabel\n\u001b[0;32m    264\u001b[0m     str_val \u001b[38;5;241m=\u001b[39m csv[fluxColumn\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 265\u001b[0m     record\u001b[38;5;241m.\u001b[39mvalues[column_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluxColumn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m     record\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mappend(record\u001b[38;5;241m.\u001b[39mvalues[column_name])\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m record\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\influxdb_client\\client\\flux_csv_parser.py:297\u001b[0m, in \u001b[0;36mFluxCsvParser._to_value\u001b[1;34m(self, str_val, column)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base64\u001b[38;5;241m.\u001b[39mb64decode(str_val)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdateTime:RFC3339\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m column\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdateTime:RFC3339Nano\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m column\u001b[38;5;241m.\u001b[39mdata_type:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_date_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m column\u001b[38;5;241m.\u001b[39mdata_type:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m# todo better type ?\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(str_val)\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1368\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser(parserinfo)\u001b[38;5;241m.\u001b[39mparse(timestr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULTPARSER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:640\u001b[0m, in \u001b[0;36mparser.parse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m     default \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mreplace(hour\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, minute\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    638\u001b[0m                                               second\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, microsecond\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 640\u001b[0m res, skipped_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown string format: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, timestr)\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:719\u001b[0m, in \u001b[0;36mparser._parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    716\u001b[0m     yearfirst \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39myearfirst\n\u001b[0;32m    718\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result()\n\u001b[1;32m--> 719\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43m_timelex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# Splits the timestr into tokens\u001b[39;00m\n\u001b[0;32m    721\u001b[0m skipped_idxs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# year/month/day list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:201\u001b[0m, in \u001b[0;36m_timelex.split\u001b[1;34m(cls, s)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mcls\u001b[39m, s):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:190\u001b[0m, in \u001b[0;36m_timelex.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 190\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:141\u001b[0m, in \u001b[0;36m_timelex.get_token\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# emit token\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# If we've already started reading a number, we keep reading\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# numbers until we find something that doesn't fit.\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnextchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    142\u001b[0m         token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nextchar\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m nextchar \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (nextchar \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\projects\\git\\antagonist\\antagonist_ml\\Lib\\site-packages\\dateutil\\parser\\_parser.py:208\u001b[0m, in \u001b[0;36m_timelex.isnum\u001b[1;34m(cls, nextchar)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Whether or not the next character is part of a word \"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nextchar\u001b[38;5;241m.\u001b[39misalpha()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misnum\u001b[39m(\u001b[38;5;28mcls\u001b[39m, nextchar):\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Whether the next character is part of a number \"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nextchar\u001b[38;5;241m.\u001b[39misdigit()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import influx_db_utils as influx\n",
    "\n",
    "machine_id = 'machine-1-1'\n",
    "\n",
    "# Read historical data from InfluxDB\n",
    "# now = datetime.datetime.now()\n",
    "# end = now - datetime.timedelta(days=1)\n",
    "# start = end - datetime.timedelta(days=20)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "start = end - datetime.timedelta(days=365)\n",
    "\n",
    "db = influx.SMDInfluxDB()\n",
    "dataframes, machines = db.read_dataset(\n",
    "    start_date=start,\n",
    "    end_date=end,\n",
    "    machine_name=machine_id,\n",
    ")\n",
    "telemetry_df = dataframes[0]  # pandas.DataFrame\n",
    "telemetry_df = telemetry_df[\n",
    "    telemetry_df.columns[1:].tolist()+['timestamp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a time window as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter up to current day to simulate the predition on the next one\n",
    "current_day = telemetry_df['timestamp'].min() + datetime.timedelta(days=32)\n",
    "next_day = current_day + datetime.timedelta(days=1)\n",
    "telemetry_df = telemetry_df[telemetry_df['timestamp']<current_day.ctime()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the model has never been trained before train it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_symptoms(symptoms_json, start, end):\n",
    "    # TODO: Move the filtering capabilities inside Antagonist\n",
    "    source_type = \"human\"\n",
    "    tags={\"machine\": machine_id}\n",
    "    \n",
    "    symptoms = []\n",
    "    for symptom in symptoms_json:\n",
    "        start_time = datetime.datetime.strptime(symptom['start-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "        end_time = datetime.datetime.strptime(symptom['end-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "\n",
    "        # TODO: Move the filtering capabilities inside Antagonist\n",
    "\n",
    "        # verify overlap between symptom interval and filters one\n",
    "        time_overlap = (start.timestamp() <= start_time <= end.timestamp()) or (start.timestamp() <= end_time <= end.timestamp())\n",
    "        if (source_type is None or symptom[\"source-type\"] == source_type) and time_overlap:\n",
    "            if tags is None or all([symptom[\"tags\"][tag] == tags[tag] for tag in tags]):\n",
    "                symptom.update({\n",
    "                    \"start-time\": start_time,\n",
    "                    \"end-time\": end_time\n",
    "                })\n",
    "                symptoms.append(symptom)\n",
    "    return symptoms\n",
    "\n",
    "\n",
    "if not anomaly_detector.is_trained():\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Get labels from Antagonist\n",
    "    ANTAGONIST_HOST = \"localhost:5001\"\n",
    "    response = requests.get(f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\")\n",
    "    response.raise_for_status()\n",
    "    symptoms = response.json()\n",
    "\n",
    "    # Format labels for the training\n",
    "    annotation_df = pd.DataFrame()\n",
    "    annotation_df['timestamp'] = telemetry_df['timestamp']\n",
    "    annotation_df['label'] = 0\n",
    "\n",
    "    for symptom in symptoms:\n",
    "        start_time_epoch = pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "        end_time_epoch = pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "        \n",
    "        # For now, skip anomalies bigger than 1 day\n",
    "        if (end_time_epoch - start_time_epoch) > 86400:\n",
    "            continue\n",
    "        \n",
    "        annotation_df.loc[\n",
    "            (annotation_df['timestamp'] >= pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\")) &\n",
    "            (annotation_df['timestamp'] <= pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\"))\n",
    "        , 'label'] = 1\n",
    "\n",
    "    # Train the model\n",
    "    anomaly_detector.train(telemetry_df, annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_anomalies = anomaly_detector.detect(telemetry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "ANTAGONIST_HOST = \"localhost:5001\"\n",
    "\n",
    "group = \"Group-1\"\n",
    "\n",
    "## Send the data to Antagonist\n",
    "for network_anomaly in network_anomalies:\n",
    "\n",
    "    # Create network anomaly label\n",
    "    net_anomaly = {\n",
    "        \"annotator\": {\n",
    "            \"name\": anomaly_detector.get_model_name(),\n",
    "            \"annotator_type\": \"algorithm\"\n",
    "        },\n",
    "        \"description\": f'Detected Network Anomaly on {machine_id} - {datetime.datetime.fromtimestamp(network_anomaly[0]).strftime(\"%Y-%m-%d at %H\")}',\n",
    "        \"state\": \"incident-potential\",\n",
    "        \"version\": 1\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"http://{ANTAGONIST_HOST}/api/rest/v1/network_anomaly\", json=net_anomaly\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    ni_uuid = response.json()\n",
    "\n",
    "    # Create network symptoms labels and link with the network incident\n",
    "    for symptom in network_anomaly[2]:\n",
    "        tags = {\n",
    "            \"machine\": machine_id,\n",
    "            \"metric\": db.get_metric_names()[symptoám[0]],\n",
    "            \"group\": group,\n",
    "        }\n",
    "\n",
    "        net_sym = {\n",
    "            'start-time': datetime.datetime.fromtimestamp(symptom[1]).strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "            'end-time': datetime.datetime.fromtimestamp(symptom[2]).strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "            \"event-id\": ni_uuid,\n",
    "            \"concern-score\": symptom[3],\n",
    "            \"confidence-score\": symptom[4],\n",
    "            \"description\": \"Symptom\",\n",
    "            \"pattern\": \"\",\n",
    "            \"tags\": tags,\n",
    "            \"annotator\": {\n",
    "                \"name\": f\"{anomaly_detector.get_model_name()}\",\n",
    "                \"annotator_type\": \"algorithm\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Persist the Symptom\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\", json=net_sym\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        symptom_uuid = response.json()\n",
    "\n",
    "        # Link the Symptom to the network anomaly\n",
    "        sym_to_net = {\"symptom-id\": symptom_uuid, \"incident-id\": ni_uuid}\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/network_anomaly/symptom\", json=sym_to_net\n",
    "        )\n",
    "        response.raise_for_status()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
