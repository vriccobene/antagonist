{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Anomaly Detector object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from demo_anomaly_detector import autoencoder_detector\n",
    "\n",
    "# If a model has been pre-trained, it will be loaded automatically\n",
    "anomaly_detector = autoencoder_detector.DemoAnomalyDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the telemetry data from InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import influx_db_utils as influx\n",
    "\n",
    "machine_id = 'machine-1-1'\n",
    "\n",
    "# Read historical data from InfluxDB (the first 20 days of data in the DB)\n",
    "now = datetime.datetime.now()\n",
    "end = now - datetime.timedelta(days=1)\n",
    "start = end - datetime.timedelta(days=20)\n",
    "\n",
    "db = influx.SMDInfluxDB()\n",
    "dataframes, machines = db.read_dataset(\n",
    "    start_date=start,\n",
    "    end_date=end,\n",
    "    machine_name=machine_id,\n",
    ")\n",
    "telemetry_df = dataframes[0]  # pandas.DataFrame\n",
    "telemetry_df = telemetry_df[\n",
    "    telemetry_df.columns[1:].tolist()+['timestamp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the model has never been trained before train it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Stored the model: ae_model_1720100977.8873696\n"
     ]
    }
   ],
   "source": [
    "def format_symptoms(symptoms_json, start, end):\n",
    "    # TODO: Move the filtering capabilities inside Antagonist\n",
    "    source_type = \"human\"\n",
    "    tags={\"machine\": machine_id}\n",
    "    \n",
    "    symptoms = []\n",
    "    for symptom in symptoms_json:\n",
    "        start_time = datetime.datetime.strptime(symptom['start-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "        end_time = datetime.datetime.strptime(symptom['end-time'], '%a, %d %b %Y %H:%M:%S %Z').timestamp()\n",
    "\n",
    "        # TODO: Move the filtering capabilities inside Antagonist\n",
    "\n",
    "        # verify overlap between symptom interval and filters one\n",
    "        time_overlap = (start.timestamp() <= start_time <= end.timestamp()) or (start.timestamp() <= end_time <= end.timestamp())\n",
    "        if (source_type is None or symptom[\"source-type\"] == source_type) and time_overlap:\n",
    "            if tags is None or all([symptom[\"tags\"][tag] == tags[tag] for tag in tags]):\n",
    "                symptom.update({\n",
    "                    \"start-time\": start_time,\n",
    "                    \"end-time\": end_time\n",
    "                })\n",
    "                symptoms.append(symptom)\n",
    "    return symptoms\n",
    "\n",
    "\n",
    "if not anomaly_detector.is_trained():\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Get labels from Antagonist\n",
    "    ANTAGONIST_HOST = \"localhost:5001\"\n",
    "    response = requests.get(f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\")\n",
    "    response.raise_for_status()\n",
    "    symptoms = response.json()\n",
    "\n",
    "    annotation_df = pd.DataFrame()\n",
    "    annotation_df['timestamp'] = telemetry_df['timestamp']\n",
    "    annotation_df['label'] = 0\n",
    "\n",
    "    for symptom in symptoms:\n",
    "        start_time_epoch = pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "        end_time_epoch = pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\").timestamp()\n",
    "        \n",
    "        if (end_time_epoch - start_time_epoch) > 86400:\n",
    "            # Skip anomalies bigger than 1 day\n",
    "            continue\n",
    "        \n",
    "        annotation_df.loc[\n",
    "            (annotation_df['timestamp'] >= pd.Timestamp(symptom['start-time'], unit=\"s\", tz=\"UTC\")) &\n",
    "            (annotation_df['timestamp'] <= pd.Timestamp(symptom['end-time'], unit=\"s\", tz=\"UTC\"))\n",
    "        , 'label'] = 1\n",
    "\n",
    "    anomaly_detector.train(telemetry_df, annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "network_incidents = anomaly_detector.detect(telemetry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "ANTAGONIST_HOST = \"localhost:5001\"\n",
    "\n",
    "group = \"Group-1\"\n",
    "\n",
    "## Send the data to Antagonist\n",
    "for network_incident in network_incidents:\n",
    "\n",
    "    # Create network incident label\n",
    "    net_inc = {\n",
    "        \"author\": {\n",
    "            \"author_type\": \"algorithm\",\n",
    "            \"name\": anomaly_detector.get_model_name(),\n",
    "            \"version\": 1,\n",
    "        },\n",
    "        \"description\": f'Detected Network Anomaly on {machine_id} - {datetime.datetime.fromtimestamp(network_incident[0]).strftime(\"%Y-%m-%d at %H\")}',\n",
    "        \"state\": \"incident-potential\",\n",
    "        \"version\": 1\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"http://{ANTAGONIST_HOST}/api/rest/v1/incident\", json=net_inc\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    ni_uuid = response.json()\n",
    "\n",
    "    # Create network symptoms labels and link with the network incident\n",
    "    for symptom in network_incident[2]:\n",
    "        tags = {\n",
    "            \"machine\": machine_id,\n",
    "            \"metric\": db.get_metric_names()[symptom[0]],\n",
    "            \"group\": group,\n",
    "        }\n",
    "\n",
    "        net_sym = {\n",
    "            'start-time': start.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "            'end-time': end.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "            \"event-id\": ni_uuid,\n",
    "            \"concern-score\": symptom[3],\n",
    "            \"confidence-score\": symptom[4],\n",
    "            \"description\": \"Symptom\",\n",
    "            \"source-name\": f\"{anomaly_detector.get_model_name()}\",\n",
    "            \"source-type\": \"algorithm\",\n",
    "            \"tags\": tags,\n",
    "            \"action\": \"drop\",\n",
    "            \"cause\": \"x\",\n",
    "            \"reason\": \"x\",\n",
    "            \"plane\": \"forwarding\",\n",
    "            \"pattern\": \"\",\n",
    "        }\n",
    "\n",
    "        # Persist the Symptom\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/symptom\", json=net_sym\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        symptom_uuid = response.json()\n",
    "\n",
    "        # Link the Symptom to the network anomaly\n",
    "        sym_to_net = {\"symptom-id\": symptom_uuid, \"incident-id\": ni_uuid}\n",
    "        response = requests.post(\n",
    "            f\"http://{ANTAGONIST_HOST}/api/rest/v1/incident/symptom\", json=sym_to_net\n",
    "        )\n",
    "        response.raise_for_status()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
